{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An algorithm to extract candidate monosemous relatives and then filter them with word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "from lxml import etree\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BasicTokenizer\n",
    "from xml.dom import minidom\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import operator\n",
    "import pymorphy2\n",
    "import json\n",
    "import copy\n",
    "import nltk\n",
    "import pickle\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loading pre-trained word2vec model\n",
    "\"\"\"\n",
    "model_2 =  Word2Vec.load(r'Taiga_1billion\\Taiga_1billion\\word2vec_win3_proza_ru.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dictionary with monosemous nouns\n",
    "\"\"\"\n",
    "with open(r'monosemous_words.pkl', 'rb') as f:\n",
    "    mono_dict_all = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "russe_synsets = {\n",
    "    \n",
    "    'замок':['N29241', 'N24173'],\n",
    "    'лук':['N41975', 'N12915'],\n",
    "    'бор':['N31212', 'N13050'],\n",
    "    'дар':['N17912', 'N31026'],\n",
    "    'двигатель':['N27057', 'N16946'],\n",
    "    'дедушка':['N20410', 'N35355'],\n",
    "    'декрет':['N13903', 'N34626'],\n",
    "    'дерево':['N40123', 'N33095'],\n",
    "    'диалог':['N34886', 'N38215'],\n",
    "    'диплом':['N35864', 'N40323', 'N20982'],\n",
    "    'доктор':['N28236', 'N30160'],\n",
    "    'доля':['N25033', 'N25297'],\n",
    "    'достижение':['N38801', 'N34449'],\n",
    "    'жестокость':['N33820', 'N41393'],\n",
    "    'жребий':['N21712', 'N25033'],\n",
    "    'затея':['N18425', 'N20053'],\n",
    "    'застой':['N25942', 'N37078'],\n",
    "    'затишье':['N39421', 'N12860'],\n",
    "    'затмение':['N19420', 'N36219'],\n",
    "    'капот': ['N13120', 'N15899'],\n",
    "    'таз': ['N14586', 'N30033'],\n",
    "    'слог': ['N15152', 'N21947'],\n",
    "    'байка': ['N16141', 'N39858'],\n",
    "    'гусеница': ['N19345', 'N21860'],\n",
    "    'стопка': ['N25286', 'N26126'],\n",
    "    'гвоздика': ['N26662', 'N31219'],\n",
    "    'крона': ['N28683', 'N30465', 'N33001', 'N37840'],#норвежская, шведская, датская,крона дерева\n",
    "    'акция': ['N29853', 'N41588'],\n",
    "    'такса': ['N35039', 'N36673'],\n",
    "    'рок': ['N36575', 'N40621']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_to_sense_definitions = {\n",
    "    \n",
    "'N29241':'ЗАМОК_СТРОЕНИЕ',\n",
    "'N24173':'ЗАМОК_ЗАПОР',\n",
    "'N12915':'ЛУК_ОРУЖИЕ',\n",
    "'N41975':'ЛУК_ОВОЩ',\n",
    "'N31212': 'БОР_ЭЛЕМЕНТ',\n",
    "'N13050': 'БОР_ЛЕС',\n",
    "'N17912':'ДАР_ВРОЖДЕННЫЙ',\n",
    "'N31026':'ДАР_ПОДАРОК',\n",
    "'N27057': 'ДВИГАТЕЛЬ_АГРЕГАТ',\n",
    "'N16946': 'ДВИГАТЕЛЬ_ДВИЖУЩ_СИЛА',\n",
    "'N20410': 'ДЕДУШКА_СТАРИК',\n",
    "'N35355': 'ДЕДУШКА_РОДСТВЕНН',\n",
    "'N13903': 'ДЕКРЕТ_ОТПУСК',\n",
    "'N34626': 'ДЕКРЕТ_ДОКУМЕНТ',\n",
    "'N40123': 'ДЕРЕВО_ДЕРЕВЦЕ',\n",
    "'N33095': 'ДЕРЕВО_ДРЕВЕСИНА',\n",
    "'N34886': 'ДИАЛОГ_БЕСЕДА',\n",
    "'N38215': 'ДИАЛОГ_МЖД_СТОРОНАМИ',\n",
    "'N35864': 'ДИПЛОМ_ВУЗА',\n",
    "'N40323': 'ДИПЛОМ_ПАМЯТНЫЙ',\n",
    "'N20982': 'ДИПЛОМ_РАБОТА',\n",
    "'N28236': 'ДОКТОР_НАУК',\n",
    "'N30160': 'ДОКТОР_ВРАЧ',\n",
    "'N25033': 'ДОЛЯ_УЧАСТЬ',\n",
    "'N25297': 'ДОЛЯ_ЧАСТЬ',\n",
    "'N38801': 'ДОСТИЖЕНИЕ_ЦЕЛИ',\n",
    "'N34449': 'ДОСТИЖЕНИЕ_УРОВНЯ',\n",
    "'N33820': 'ЖЕСТОКОСТЬ_БЕСПОЩАДНОСТЬ',\n",
    "'N41393': 'ЖЕСТОКОСТЬ_ОБРАЩЕНИЕ',\n",
    "'N21712': 'ЖРЕБИЙ_РЕШЕНИЕ',\n",
    "'N25033': 'ЖРЕБИЙ_СУДЬБА',\n",
    "'N18425': 'ЗАТЕЯ_ЗАБАВА',\n",
    "'N20053': 'ЗАТЕЯ_НАЧИНАНИЕ',\n",
    "'N25942': 'ЗАСТОЙ_ЗАСТОЙН_ЯВЛЕНИЕ',\n",
    "'N37078': 'ЗАСТОЙ_СТАГНАЦИЯ_РАЗВИТ',\n",
    "'N39421': 'ЗАТИШЬЕ_СНИЖ_АКТИВНОСТИ',\n",
    "'N12860': 'ЗАТИШЬЕ_БЕЗВЕТР_ТИШЬ',\n",
    "'N19420': 'ЗАТМЕНИЕ_ОДУРЕНИЕ',\n",
    "'N36219': 'ЗАТМЕНИЕ_СВЕТИЛА',\n",
    "    'N13120':'КАПОТ_ОДЕЖДА',\n",
    "    'N15899': 'КАПОТ_МАШИНЫ',\n",
    "    'N14586':'ТАЗ_КОСТЬ',\n",
    "    'N30033': 'ТАЗ_ПОСУДА',\n",
    "    'N15152':'СЛОГ_ЗВУК', \n",
    "    'N21947': 'СЛОГ_СТИЛЬ',\n",
    "    'N16141': 'БАЙКА_ЛОЖЬ',\n",
    "    'N39858': 'БАЙКА_ТКАНЬ',\n",
    "    'N19345': 'ГУСЕНИЦА_МЕХАНИЗМ',\n",
    "    'N21860': 'ГУСЕНИЦА_ЛИЧИНКА',\n",
    "    'N25286': 'СТОПКА_КУЧА',\n",
    "    'N26126': 'СТОПКА_ПОСУДА',\n",
    "    'N26662': 'ГВОЗДИКА_ПРИПРАВА',\n",
    "    'N31219': 'ГВОЗДИКА_РАСТЕНИЕ',\n",
    "    'N28683': 'КРОНА_ДЕНЬГИ',\n",
    "    'N30465': 'КРОНА_ДЕНЬГИ',\n",
    "    'N33001': 'КРОНА_ДЕНЬГИ',\n",
    "    'N37840': 'КРОНА_ДЕРЕВА',\n",
    "    'N29853': 'АКЦИЯ_КОМПАНИИ',\n",
    "    'N41588': 'АКЦИЯ_ДЕЙСТВИЕ',\n",
    "    'N35039': 'ТАКСА_СОБАКА',\n",
    "    'N36673': 'ТАКСА_ОПЛАТА',\n",
    "    'N36575': 'РОК_МУЗЫКА',\n",
    "    'N40621': 'РОК_СУДЬБА',\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating dictionary with close synset realtions\n",
    "\"\"\"\n",
    "relations_file_names = [r'Synsets_xml\\synset_relations.N.xml', r'Synsets_xml\\synset_relations.A.xml', \n",
    "                       r'Synsets_xml\\synset_relations.V.xml']\n",
    "\n",
    "relatives_dict_new = {}\n",
    "\n",
    "for file in relations_file_names:\n",
    "    doc_N = etree.parse(file)\n",
    "    root = doc_N.getroot()\n",
    "    sense_list = []\n",
    "    par_id_prev = '' #previous parent_id\n",
    "    num=0\n",
    "    for child in tqdm_notebook(root):\n",
    "\n",
    "        if par_id_prev == child.attrib['parent_id']:\n",
    "            if child.attrib['name'] in relatives_dict_new[child.attrib['parent_id']].keys():\n",
    "                relatives_dict_new[child.attrib['parent_id']][child.attrib['name']].append(child.attrib['child_id'])\n",
    "            else:\n",
    "                relatives_dict_new[child.attrib['parent_id']][child.attrib['name']]=[child.attrib['child_id']]\n",
    "                par_id_prev = child.attrib['parent_id']\n",
    "\n",
    "        else:\n",
    "            relatives_dict_new[child.attrib['parent_id']]={}\n",
    "\n",
    "            relatives_dict_new[child.attrib['parent_id']][child.attrib['name']]=[child.attrib['child_id']]\n",
    "            par_id_prev = child.attrib['parent_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_up_list(d1, d2, n=1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to create multi-level dictionary with relatives for a target synset\n",
    "    Args:\n",
    "    \n",
    "    d1 (dict): dictionary with synset realtions\n",
    "    \n",
    "    d2 (dict): dictionary with close relatives to a target synset\n",
    "    \n",
    "    n (int): number of steps to extract relatives\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    dictionary with all relatives and relations conneting them and target synset\n",
    "    \"\"\"\n",
    "    \n",
    "    out = {k: {k1: {i: d1[i] for i in val1}\n",
    "        for k1,val1 in val.items()}\n",
    "        for k,val in d2.items()}\n",
    "    if n==1:\n",
    "        return out\n",
    "    else:\n",
    "        out1 = {k:{k1:dict_up_list(d1,val1,n-1)\n",
    "        for k1,val1 in val.items()}\n",
    "        for k,val in out.items()}\n",
    "        return out1\n",
    "    \n",
    "def relats_def(rel_lst, mono_dict_all = mono_dict_all, synset_out = False, target_word=''):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to extract words from synsets\n",
    "    Args: \n",
    "    \n",
    "    rel_lst (list): list of the relatives expressed as synset names\n",
    "    \n",
    "    mono_dict_all (dict): dictionary with monosemous words\n",
    "    \n",
    "    synset_out (bool): if True then we will output all the relative words (without\n",
    "    dividing them into monosemous and polysemous)\n",
    "    \n",
    "    target_word (str): target polysemous word\n",
    "    \n",
    "    Returns:\n",
    "    either list of monosemous and polysemous relative words or list with all relatives\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    rel_list_def = []\n",
    "    rel_def_mono = []\n",
    "    rel_def_mult = []\n",
    "    rel_dict = {}\n",
    "    \n",
    "    \n",
    "    if synset_out==False:\n",
    "        for i in rel_lst:\n",
    "            rel_list_def.extend(copy.deepcopy(synset_words[i]))\n",
    "    else:\n",
    "        for i in rel_lst:\n",
    "            rel_dict[i] = normalize_words(copy.deepcopy(synset_words[i]), excl_root=True, target=target_word)\n",
    "        return rel_dict\n",
    "     \n",
    "    for i in rel_list_def:\n",
    "        if i in mono_dict_all.keys():\n",
    "            rel_def_mono.append(i)\n",
    "        else:\n",
    "            rel_def_mult.append(i)\n",
    "\n",
    "    return rel_def_mono, rel_def_mult\n",
    "\n",
    "def normalize_words(syn_1, excl_root = True, target=''):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to lemmatize list of words\n",
    "    Args:\n",
    "    \n",
    "    syn_1 (list): list of words\n",
    "    \n",
    "    excl_root (bool): whether to exclude target word from a final list or not\n",
    "    \n",
    "    target (str): target word (in case we need to exclude it)\n",
    "    \n",
    "    Returns:\n",
    "    list of lemmatized words with #-symbol instead of whitespaces\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    norm_synset = [] #lemmas of the monosemous words\n",
    "    for i in tqdm_notebook(syn_1):\n",
    "        if excl_root==True:\n",
    "            if i!=target:\n",
    "                tokens = i.split()\n",
    "                txt = [morph.parse(token)[0].normal_form.strip(' ') for token in tokens]\n",
    "                norm_synset.append('#'.join(txt))\n",
    "        else:\n",
    "            tokens = i.split()\n",
    "            txt = [morph.parse(token)[0].normal_form.strip(' ') for token in tokens]\n",
    "            norm_synset.append('#'.join(txt))\n",
    "    return norm_synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relatives_extraction(multinom_word, synset_num, path_to_save_file, nest = False):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    An algorithm of monosemous relative extraction\n",
    "    \n",
    "    Args:\n",
    "    multinom_word (str): the target polysemous word\n",
    "    \n",
    "    synset_num (str): the synset the target word belongs to\n",
    "    \n",
    "    path_to_save_file (str): path where output files with relatives will be stored\n",
    "    \n",
    "    nest (bool): whether to use only close candidate relatives instead of the ones whithin 4-step relation path\n",
    "    \n",
    "    Returns:\n",
    "    files with relatives and their weights\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    global target_word\n",
    "    target_word = multinom_word\n",
    "    \n",
    "    file_name = path_to_save_file+target_word+'_'+synset_num+'.txt'\n",
    "    file_name_csv = path_to_save_file+target_word+'_'+synset_num+'.csv'\n",
    "\n",
    "\n",
    "    root_concept = synset_num\n",
    "    \n",
    "    dict_2 = copy.deepcopy(relatives_dict_new[root_concept])\n",
    "    ex_relats = {root_concept:dict_2}\n",
    "\n",
    "    # collecting relatives within 4-step path\n",
    "    relatives_3_step = dict_up_list(relatives_dict_new, ex_relats, n=3) \n",
    "\n",
    "    close_relatives = set()\n",
    "    one_step_relatives = set()\n",
    "    two_step_relatives  = set() \n",
    "    three_step_relatives = set()\n",
    "\n",
    "    # dictionaries that contain information about relation types between synsets\n",
    "    relation_list_0 = {}\n",
    "    relation_list_1 = {}\n",
    "    relation_list_2 = {}\n",
    "    relation_list_3 = {}\n",
    "    \n",
    "    # list of the relations under consideration\n",
    "    available_relations = ['hypernym', 'hyponym', 'instance hyponym', 'instance hypernym']\n",
    "\n",
    "\n",
    "    #synset_inself\n",
    "    root_syn = copy.deepcopy(synset_words[root_concept])\n",
    "    root_syn.remove(word)\n",
    "\n",
    "\n",
    "    for key, val in relatives_3_step.items():\n",
    "        for key_2, val_2 in val.items():\n",
    "            if key_2 in available_relations:\n",
    "                relation = key_2\n",
    "                for key_3, val_3 in val_2.items():\n",
    "                    \n",
    "                    # directly connected relatives, e.g.hyponyms, hypernyms, one step\n",
    "                    close_relatives.add(key_3)\n",
    "                        \n",
    "                    relation_list_0[key_3] = relation\n",
    "\n",
    "                    for key_4, val_4 in val_3.items():\n",
    "                        relation_1 = key_4\n",
    "                        if key_4 in available_relations:\n",
    "                            for key_5, val_5 in val_4.items():\n",
    "                                \n",
    "                                # checking whether the synset is not the one we already put into list\n",
    "                                if key_5!=root_concept and key_5 not in close_relatives:\n",
    "                                    # relatives at two-step path\n",
    "                                    one_step_relatives.add(key_5)\n",
    "\n",
    "                                    relation_list_1[key_5] = relation+'_'+relation_1\n",
    "                                    \n",
    "                                    for key_6, val_6 in val_5.items():\n",
    "                                        relation_2 = key_6\n",
    "                                        if key_6 == relation_1 :\n",
    "                                            for key_7, val_7 in val_6.items():\n",
    "                                                if key_7!=key_5 and key_7  not in one_step_relatives:\n",
    "                                                    # relatives at three-step path\n",
    "                                                    two_step_relatives.add(key_7)\n",
    "                                                    \n",
    "                                                    relation_list_2[key_7] = relation+'_'+relation_1+'_'+relation_2\n",
    "\n",
    "                                                    for key_8, val_8 in val_7.items():\n",
    "                                                        relation_3 =key_8\n",
    "                                                        if key_8 == relation_1:\n",
    "                                                            # relatives at four-step path\n",
    "                                                            three_step_relatives.update([i for i in val_8 if i not in list(two_step_relatives)+list(one_step_relatives)])\n",
    "\n",
    "                                                            for k in val_8:\n",
    "                                                                if k not in list(close_relatives)+list(two_step_relatives)+list(one_step_relatives):\n",
    "                                                                    relation_list_3[k] = relation+'_'+relation_1+'_'+relation_2+'_'+relation_3\n",
    "\n",
    "    # cleaning synset lists so that they won't contain synsets from the previous steps\n",
    "    one_step_relatives -= close_relatives.intersection(one_step_relatives)\n",
    "    two_step_relatives -= two_step_relatives.intersection(one_step_relatives)\n",
    "    two_step_relatives -= two_step_relatives.intersection(close_relatives)\n",
    "    three_step_relatives = three_step_relatives - two_step_relatives.intersection(three_step_relatives)\n",
    "    three_step_relatives -= one_step_relatives.intersection(three_step_relatives)\n",
    "    three_step_relatives -= close_relatives.intersection(three_step_relatives)\n",
    "    \n",
    "    \n",
    "    relation_list_1 = {your_key: relation_list_1[your_key] for your_key in one_step_relatives}\n",
    "    relation_list_2 = {your_key: relation_list_2[your_key] for your_key in two_step_relatives}\n",
    "    relation_list_3 = {your_key: relation_list_3[your_key] for your_key in three_step_relatives}\n",
    "\n",
    "    \n",
    "    relation_list = {**relation_list_0, **relation_list_1}\n",
    "    relation_list = {**relation_list, **relation_list_2}\n",
    "    relation_list = {**relation_list, **relation_list_3}\n",
    "    \n",
    "\n",
    "    # obtaining monosemous and polysemous words from the synsets of different distances\n",
    "    close_rel_def_mono, close_rel_def_mult = relats_def(list(close_relatives))\n",
    "    one_step_def_mono, one_step_def_mult = relats_def(list(one_step_relatives))\n",
    "    two_step_def_mono, two_step_def_mult = relats_def(list(two_step_relatives))\n",
    "    three_step_def_mono, three_step_def_mult = relats_def(list(three_step_relatives))\n",
    "    \n",
    "    # leaving only monosemous words from a target synset\n",
    "    root_syn_mono = []\n",
    "    for i in root_syn:\n",
    "        if i in mono_dict_all.keys():\n",
    "            root_syn_mono.append(i)\n",
    "\n",
    "\n",
    "    #choosing the scope of candidate monosemous relatives \n",
    "    if nest == True:\n",
    "    #we take only close relatives and a synset as candidates to monosemous relatives\n",
    "        all_relatives_list = list(set(close_rel_def_mono))+root_syn_mono\n",
    "    \n",
    "    else:\n",
    "        # candidate monosemous relatives whithin 4-step relation path\n",
    "        all_relatives_list = list(set(close_rel_def_mono))+list(set(one_step_def_mono))+\\\n",
    "        list(set(two_step_def_mono))+list(set(three_step_def_mono))+root_syn_mono\n",
    "    \n",
    "    # normalizing obtained list\n",
    "    all_relatives_list_norm = normalize_words(all_relatives_list)\n",
    "    \n",
    "    # one step relatives\n",
    "    close_synsets = relats_def(close_relatives, synset_out=True, target_word=target_word)\n",
    "    # two step relatives\n",
    "    one_synsets = relats_def(one_step_relatives, synset_out=True, target_word=target_word)\n",
    "    # one- and two-step relatives\n",
    "    relats_0_1 = relats_def(list(one_step_relatives)+list(close_relatives)+[root_concept],\n",
    "                            synset_out=True, target_word=target_word) \n",
    "    \n",
    "    # defining a synset nest\n",
    "    relatives_first_circle = root_syn+list(set(close_rel_def_mono))+\\\n",
    "    list(set(close_rel_def_mult))+list(set(one_step_def_mult)) +list(set(one_step_def_mono))\n",
    "\n",
    "    relatives_first_circle = normalize_words(relatives_first_circle)\n",
    "    \n",
    "    \n",
    "    words = []\n",
    "    res_100 = []\n",
    "    res_100_no_weights = []\n",
    "    res_list = []\n",
    "    weight_list = {}\n",
    "\n",
    "    # for every candidate in candidate list\n",
    "    for i in tqdm_notebook(all_relatives_list_norm):\n",
    "\n",
    "        word = i.split()[0]\n",
    "        sim={}\n",
    "        if word in model_2.wv.vocab:\n",
    "            res = model_2.wv.most_similar(positive=word, topn=100)\n",
    "            for j in res:\n",
    "                sim[j[0]] = j[1]\n",
    "\n",
    "        else:\n",
    "            res = 'word is not in w2v vocab'\n",
    "\n",
    "        words.append(word)\n",
    "        res_100.append(res)\n",
    "        res_100_no_weights.append(sim)\n",
    "    \n",
    "    similar_100_action = pd.DataFrame(columns=['word', '100_similar_words'])\n",
    "    similar_100_action['word'] = pd.Series(words)\n",
    "\n",
    "    similar_100_action['100_similar_words'] = pd.Series(res_100)\n",
    "    \n",
    "    similar_100_action['weight_sum'] = \"\"\n",
    "\n",
    "    rows_to_delete = []\n",
    "    rows_to_delete_2 = []\n",
    "    totally_delete = []\n",
    "\n",
    "    for i in range(len(similar_100_action)):\n",
    "        intersect = []\n",
    "        # weight of the candidate\n",
    "        weight = 0\n",
    "        # weights of the synsets\n",
    "        weight_dict = {}\n",
    "        for tup in similar_100_action['100_similar_words'].iloc[i]:\n",
    "            \n",
    "            # if this word is included in a nest\n",
    "            if tup[0] in relatives_first_circle:\n",
    "                \n",
    "                for key, val in relats_0_1.items():\n",
    "\n",
    "                    if tup[0] in val and tup[0]!=similar_100_action['word'].iloc[i]:\n",
    "\n",
    "                        if key in weight_dict.keys():\n",
    "                                # updating the values in case tthe new one is higher\n",
    "                                if tup[1] > weight_dict[key]:\n",
    "                                    weight_dict[key]=tup[1]\n",
    "                        else:\n",
    "                            weight_dict[key]=tup[1]\n",
    "\n",
    "                intersect.append(tup[0])\n",
    "            weight = sum(weight_dict.values())\n",
    "            \n",
    "\n",
    "\n",
    "        if intersect == []:\n",
    "            intersect = 'No intersection'\n",
    "        \n",
    "        # filtering words according to weight and the number of relatives found\n",
    "        if weight <= 1.5:  \n",
    "            rows_to_delete.append(i)\n",
    "        elif weight <= 0.9:  \n",
    "            rows_to_delete_2.append(i)\n",
    "        if weight==0:\n",
    "            totally_delete.append(i)\n",
    "    \n",
    "        if len(similar_100_action)<=3:\n",
    "            rows_to_delete = []\n",
    "        elif len(rows_to_delete)>=len(similar_100_action)-3:\n",
    "            if len(rows_to_delete_2)<=len(similar_100_action)-3:\n",
    "                rows_to_delete = rows_to_delete_2\n",
    "            else:\n",
    "                rows_to_delete = []\n",
    "\n",
    "\n",
    "        similar_100_action['weight_sum'].iloc[i] = weight\n",
    "    \n",
    "    similar_100_action = similar_100_action.drop(rows_to_delete+totally_delete)\n",
    "    similar_100_action = similar_100_action.reset_index(drop=True)\n",
    "    \n",
    "    similar_100_action.to_csv(file_name_csv, sep=';', encoding='utf-8-sig')\n",
    "    \n",
    "    return None\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_csv_folder = r'C:\\Users\\Filtering_relatives_TOLOKA_TAIGA\\relatives_'\n",
    "\n",
    "for key, val in russe_synsets.items():\n",
    "    \n",
    "    for syns in val:\n",
    "        \n",
    "        relatives_extraction(key, syns, path_to_csv_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mapping from polysemous word to its relatives\n",
    "\"\"\"\n",
    "\n",
    "csv_dir = r''\n",
    "tuples = {}\n",
    "\n",
    "for file in os.listdir(csv_dir):\n",
    "    \n",
    "    if file == 'relatives_доля_N25033.csv' or file == 'доля_N25033.csv':\n",
    "        \n",
    "        relat_name = 'ДОЛЯ_УЧАСТЬ'\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        relat_name = file.replace('relatives_', '').replace('.csv', '')\n",
    "        relat_name = mapping_to_sense_definitions[relat_name.split('_')[1]]\n",
    "    \n",
    "    df = pd.read_csv(csv_dir+file, sep=';')\n",
    "    \n",
    "    \n",
    "    if df.empty == True:\n",
    "        print(relat_name)\n",
    "        continue\n",
    "\n",
    "    tuples[relat_name] = {}\n",
    "    for i in tqdm_notebook(range(len(df))):\n",
    "        tuples[relat_name][df.word.iloc[i]] = df.weight_sum.iloc[i]\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
