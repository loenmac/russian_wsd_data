{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "from lxml import etree\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BasicTokenizer\n",
    "from xml.dom import minidom\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import operator\n",
    "import pymorphy2\n",
    "import json\n",
    "import copy\n",
    "import nltk\n",
    "import pickle\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loading dictionary with all monosemous nouns\n",
    "\"\"\"\n",
    "with open(r'monosemous_words.pkl', 'rb') as f:\n",
    "    mono_dict_all = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing of a Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Opening file with raw corpus\n",
    "\"\"\"\n",
    "path_to_all_texts = r'Taiga_1billion\\Taiga_1billion\\taiga_all_proza_ru.txt'\n",
    "\n",
    "with open(path_to_all_texts, 'r', encoding='utf8') as f:\n",
    "    corpora = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loading Stopwords\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with open(r'stopwords.txt', 'r', encoding='utf8') as f:\n",
    "    stopwords = f.readlines()\n",
    "stopwords_rus = []\n",
    "for i in stopwords:\n",
    "    if i!='\\n':\n",
    "        stopwords_rus.append(i.strip('\\n'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text_file, path_to_save):\n",
    "    \n",
    "    corpus = []\n",
    "\n",
    "    sentences = nltk.sent_tokenize(text_file, language=\"russian\")\n",
    "\n",
    "    print('Number of Sentences: ', len(sentences))\n",
    "\n",
    "    with open(path_to_save, 'a', encoding='utf8') as f:\n",
    "        for sent in tqdm_notebook(sentences):\n",
    "            brief_cleaning = re.sub(\"[^А-Яа-яёA-Za-z-']+\", ' ', sent)\n",
    "            tokens = [w.lower() for w in brief_cleaning.split() if w.lower() not in stopwords_rus]     \n",
    "            txt = [morph.parse(token.lower().strip())[0].normal_form.strip(' ') for token in tokens]\n",
    "        \n",
    "            f.write(' '.join(txt)+'\\n')\n",
    "            \n",
    "            corpus.append(txt)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = preprocessing(corpora, 'Taiga_1billion\\Taiga_1billion\\proza_ru_preproc_split.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Saving the preprocessed corpus\n",
    "\"\"\"\n",
    "path_to_preprocessed_corpus = r'C:/Users/Ангелина/Python_codes/processed_corpora_taiga_proza_ru.txt'\n",
    "with open(path_to_preprocessed_corpus, 'w', encoding='utf8') as f:\n",
    "    for doc in documents:\n",
    "        f.write(' '.join(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing multiword expressions in corpus\n",
    "### We will replace whitespace in frequent multiword expressions with the symbol \"#\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating dictionaries with words from RuWordNet Thesaurus\n",
    "\"\"\"\n",
    "senses_file_name = r'Synsets_xml\\senses.N.xml'\n",
    "synsets_file_name = r'Synsets_xml\\synsets.N.xml'\n",
    "\n",
    "senses_dict = {} #mapping id - word\n",
    "synset_dict = {} #mapping word - synset it belongs to\n",
    "doc_N = etree.parse(senses_file_name)\n",
    "root = doc_N.getroot()\n",
    "for child in root:\n",
    "    name = child.attrib['name'].lower()#.replace(',', '')\n",
    "    if name in mono_dict_all.keys():\n",
    "        senses_dict[child.attrib['id']] = child.attrib['name'].lower()\n",
    "        synset_dict[child.attrib['name'].lower()] = child.attrib['synset_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synset_words = {} #mapping synset-all its words\n",
    "senses_file_name_N = r'Synsets_xml\\senses.N.xml'\n",
    "senses_file_name_V = r'Synsets_xml\\senses.V.xml'\n",
    "senses_file_name_A = r'Synsets_xml\\senses.A.xml'\n",
    "\n",
    "for senses_file in [senses_file_name_N, senses_file_name_V, senses_file_name_A]:\n",
    "\n",
    "    doc_N = etree.parse(senses_file)\n",
    "    root = doc_N.getroot()\n",
    "    for child in root:\n",
    "\n",
    "        if child.attrib['synset_id'] not in synset_words.keys():\n",
    "            synset_words[child.attrib['synset_id']] = []\n",
    "            synset_words[child.attrib['synset_id']].append(child.attrib['name'].lower())\n",
    "        else:\n",
    "            synset_words[child.attrib['synset_id']].append(child.attrib['name'].lower())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating a list with multiword keywords\n",
    "\"\"\"\n",
    "\n",
    "multiword_keys = []\n",
    "for key in tqdm_notebook(synset_dict.keys()):\n",
    "    if len(key.split())>1:\n",
    "        multiword_keys.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Getting lemmas from multiword keywords\n",
    "\"\"\"\n",
    "\n",
    "normal_multiword_keys = []\n",
    "for word in multiword_keys:\n",
    "    \n",
    "    lemmas = [morph.parse(w)[0].normal_form for w in word.split()]\n",
    "    lemmas_clean = [re.sub(r'[-,]+', ' ', i) for i in lemmas]\n",
    "    \n",
    "    new_lemmas = re.sub('( )+', ' ', ' '.join(lemmas_clean))\n",
    "    normal_multiword_keys.append(new_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Counting the number of occurencies of multiword expressions\n",
    "\"\"\"\n",
    "path_to_save_counts = r'Taiga_1billion\\Taiga_1billion\\multiword_expressions_counts.txt'\n",
    "search_results = []\n",
    "s=0\n",
    "\n",
    "for key in tqdm_notebook(normal_multiword_keys):\n",
    "        \n",
    "    if s % 500 == 0:\n",
    "        with open(path_to_save_counts, 'w', encoding='utf8') as f:\n",
    "            for i in search_results:\n",
    "                for j in i:\n",
    "                    f.write(j+'\\n')\n",
    "    \n",
    "    \n",
    "    multiword_pat = re.compile(r'[ |\\n]'+key.strip('\\n')+'[ |\\n]')\n",
    "    \n",
    "    found_res = re.findall(multiword_pat, processed_corpus)\n",
    "\n",
    "    if found_res:\n",
    "        search_results.append(found_res)\n",
    "        \n",
    "    s+=1\n",
    "    \n",
    "with open(path_to_save_counts, 'w', encoding='utf8') as f:\n",
    "    for i in search_results:\n",
    "        for j in i:\n",
    "            f.write(j+'\\n')\n",
    "            \n",
    "# Converting counts to dictionary\n",
    "\n",
    "global_counter = {}\n",
    "\n",
    "for i in search_results:\n",
    "    global_counter[i[0].strip()] = int(len(i))\n",
    "sorted_x = sorted(global_counter.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Replacing frequent multiword expressions with the ones with #-sign\n",
    "\"\"\"\n",
    "\n",
    "path_to_new_corpus = r'Taiga_1billion\\Taiga_1billion\\proza_ru_split_processed_multiwords.txt'\n",
    "\n",
    "\n",
    "for num, key in enumerate(tqdm_notebook(sorted_x)):\n",
    "    \n",
    "    if num%50==0:\n",
    "        with open(path_to_new_corpus, 'w', encoding='utf8') as f:\n",
    "            f.write(processed_corpus)\n",
    "    \n",
    "    keyword = key[0]\n",
    "    \n",
    "    keyword = re.sub(r'[-,]+', ' ', keyword)\n",
    "    keyword = re.sub('( )+', ' ', keyword)\n",
    "    keyword = keyword.replace(' ', '#')\n",
    "        \n",
    "    processed_corpus = re.sub(r'([ |\\n])'+key[0]+'([ |\\n])', r'\\1'+keyword+r'\\2', processed_corpus)\n",
    "\n",
    "with open(path_to_new_corpus, 'w', encoding='utf8') as f:\n",
    "    f.write(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
