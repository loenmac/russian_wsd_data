{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "from lxml import etree\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BasicTokenizer\n",
    "from xml.dom import minidom\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import operator\n",
    "import pymorphy2\n",
    "import json\n",
    "import copy\n",
    "import nltk\n",
    "import pickle\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting samples with candidates from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_to_sense_definitions = {\n",
    "    \n",
    "'N29241':'ЗАМОК_СТРОЕНИЕ',\n",
    "'N24173':'ЗАМОК_ЗАПОР',\n",
    "'N12915':'ЛУК_ОРУЖИЕ',\n",
    "'N41975':'ЛУК_ОВОЩ',\n",
    "'N31212': 'БОР_ЭЛЕМЕНТ',\n",
    "'N13050': 'БОР_ЛЕС',\n",
    "'N17912':'ДАР_ВРОЖДЕННЫЙ',\n",
    "'N31026':'ДАР_ПОДАРОК',\n",
    "'N27057': 'ДВИГАТЕЛЬ_АГРЕГАТ',\n",
    "'N16946': 'ДВИГАТЕЛЬ_ДВИЖУЩ_СИЛА',\n",
    "'N20410': 'ДЕДУШКА_СТАРИК',\n",
    "'N35355': 'ДЕДУШКА_РОДСТВЕНН',\n",
    "'N13903': 'ДЕКРЕТ_ОТПУСК',\n",
    "'N34626': 'ДЕКРЕТ_ДОКУМЕНТ',\n",
    "'N40123': 'ДЕРЕВО_ДЕРЕВЦЕ',\n",
    "'N33095': 'ДЕРЕВО_ДРЕВЕСИНА',\n",
    "'N34886': 'ДИАЛОГ_БЕСЕДА',\n",
    "'N38215': 'ДИАЛОГ_МЖД_СТОРОНАМИ',\n",
    "'N35864': 'ДИПЛОМ_ВУЗА',\n",
    "'N40323': 'ДИПЛОМ_ПАМЯТНЫЙ',\n",
    "'N20982': 'ДИПЛОМ_РАБОТА',\n",
    "'N28236': 'ДОКТОР_НАУК',\n",
    "'N30160': 'ДОКТОР_ВРАЧ',\n",
    "'N25033': 'ДОЛЯ_УЧАСТЬ',\n",
    "'N25297': 'ДОЛЯ_ЧАСТЬ',\n",
    "'N38801': 'ДОСТИЖЕНИЕ_ЦЕЛИ',\n",
    "'N34449': 'ДОСТИЖЕНИЕ_УРОВНЯ',\n",
    "'N33820': 'ЖЕСТОКОСТЬ_БЕСПОЩАДНОСТЬ',\n",
    "'N41393': 'ЖЕСТОКОСТЬ_ОБРАЩЕНИЕ',\n",
    "'N21712': 'ЖРЕБИЙ_РЕШЕНИЕ',\n",
    "'N25033': 'ЖРЕБИЙ_СУДЬБА',\n",
    "'N18425': 'ЗАТЕЯ_ЗАБАВА',\n",
    "'N20053': 'ЗАТЕЯ_НАЧИНАНИЕ',\n",
    "'N25942': 'ЗАСТОЙ_ЗАСТОЙН_ЯВЛЕНИЕ',\n",
    "'N37078': 'ЗАСТОЙ_СТАГНАЦИЯ_РАЗВИТ',\n",
    "'N39421': 'ЗАТИШЬЕ_СНИЖ_АКТИВНОСТИ',\n",
    "'N12860': 'ЗАТИШЬЕ_БЕЗВЕТР_ТИШЬ',\n",
    "'N19420': 'ЗАТМЕНИЕ_ОДУРЕНИЕ',\n",
    "'N36219': 'ЗАТМЕНИЕ_СВЕТИЛА',\n",
    "    'N13120':'КАПОТ_ОДЕЖДА',\n",
    "    'N15899': 'КАПОТ_МАШИНЫ',\n",
    "    'N14586':'ТАЗ_КОСТЬ',\n",
    "    'N30033': 'ТАЗ_ПОСУДА',\n",
    "    'N15152':'СЛОГ_ЗВУК', \n",
    "    'N21947': 'СЛОГ_СТИЛЬ',\n",
    "    'N16141': 'БАЙКА_ЛОЖЬ',\n",
    "    'N39858': 'БАЙКА_ТКАНЬ',\n",
    "    'N19345': 'ГУСЕНИЦА_МЕХАНИЗМ',\n",
    "    'N21860': 'ГУСЕНИЦА_ЛИЧИНКА',\n",
    "    'N25286': 'СТОПКА_КУЧА',\n",
    "    'N26126': 'СТОПКА_ПОСУДА',\n",
    "    'N26662': 'ГВОЗДИКА_ПРИПРАВА',\n",
    "    'N31219': 'ГВОЗДИКА_РАСТЕНИЕ',\n",
    "    'N28683': 'КРОНА_ДЕНЬГИ',\n",
    "    'N30465': 'КРОНА_ДЕНЬГИ',\n",
    "    'N33001': 'КРОНА_ДЕНЬГИ',\n",
    "    'N37840': 'КРОНА_ДЕРЕВА',\n",
    "    'N29853': 'АКЦИЯ_КОМПАНИИ',\n",
    "    'N41588': 'АКЦИЯ_ДЕЙСТВИЕ',\n",
    "    'N35039': 'ТАКСА_СОБАКА',\n",
    "    'N36673': 'ТАКСА_ОПЛАТА',\n",
    "    'N36575': 'РОК_МУЗЫКА',\n",
    "    'N40621': 'РОК_СУДЬБА',\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mapping from polysemous word to its relatives\n",
    "\"\"\"\n",
    "\n",
    "csv_dir = r''\n",
    "tuples = {}\n",
    "\n",
    "for file in os.listdir(csv_dir):\n",
    "    \n",
    "    if file == 'relatives_доля_N25033.csv':\n",
    "        \n",
    "        relat_name = 'ДОЛЯ_УЧАСТЬ'\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        relat_name = file.replace('relatives_', '').replace('.csv', '')\n",
    "        relat_name = mapping_to_sense_definitions[relat_name.split('_')[1]]\n",
    "    \n",
    "    df = pd.read_csv(csv_dir+file, sep=';')\n",
    "    \n",
    "    \n",
    "    if df.empty == True:\n",
    "        print(relat_name)\n",
    "        continue\n",
    "\n",
    "    tuples[relat_name] = {}\n",
    "    for i in tqdm_notebook(range(len(df))):\n",
    "        tuples[relat_name][df.word.iloc[i]] = df.weight_sum.iloc[i]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_toa_processed_corpus = r'C:\\Users\\Taiga_1billion\\Taiga_1billion\\proza_ru_split_processed_multiwords_proper_hash_09_03.txt'\n",
    "with open(path_to_processed_corpus, 'r', encoding='utf8') as f:\n",
    "    processed_corpus = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_contexts_wout_replacements = r'C:\\Users\\Toloka_contexts_to_replacement_taiga_proza_ru\\\\'\n",
    "\n",
    "for key, val in tqdm(tuples.items()):\n",
    "    \n",
    "    \n",
    "    \n",
    "    for key_2 in tqdm_notebook(val.keys()):\n",
    "        \n",
    "        target = key_2\n",
    "        filename = key+'_'+key_2+'.txt'\n",
    "\n",
    "        if os.path.isfile(path_to_contexts_wout_replacements+filename):\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "\n",
    "            with open(path_to_contexts_wout_replacements + filename, \n",
    "                      'w', encoding='utf8')as f:\n",
    "                for text in re.findall(r'((\\n[а-яё# -]+\\s|\\n)'+target+'(\\s[а-яё# -]+\\n|\\n))', processed_corpus):\n",
    "                    f.write(text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Corpus with Replaced Monosemous Relatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Making Percent Weights\n",
    "\"\"\"\n",
    "for poly_w, relatives in tuples.items():\n",
    "    \n",
    "    tuples[poly_w] = {}\n",
    "    \n",
    "    all_weights = []\n",
    "    \n",
    "    for mono_w, weight in relatives.items():\n",
    "        \n",
    "        all_weights.append(weight)\n",
    "    \n",
    "    sum_weight = sum(all_weights)\n",
    "    \n",
    "    for mono_w, weight in relatives.items():\n",
    "        \n",
    "        rounded_weight = round(weight/sum_weight, 3)\n",
    "        if rounded_weight == 0:\n",
    "            rounded_weight = 0.001\n",
    "        \n",
    "        tuples[poly_w][mono_w] = rounded_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_total = []\n",
    "path_to_corpus_with_replaced_relats = r'C:\\Users\\Taiga_proza_ru_replaced_contexts\\\\'\n",
    "\n",
    "desired_number_examples = 1300\n",
    "\n",
    "for key, val in tuples.items():\n",
    "    \n",
    "    print(key)\n",
    "    \n",
    "    corpus = []\n",
    "    total_len = 0\n",
    "    \n",
    "    poly_word = key.split('_')[0].lower()\n",
    "    \n",
    "    x = tuples[key]\n",
    "    sorted_x = sorted(x.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    for tup in sorted_x:\n",
    "        \n",
    "        candidate = tup[0]\n",
    "        proportion = tup[1]\n",
    "        num_ex = int(round(proportion * desired_number_examples))\n",
    "        if num_ex == 0:\n",
    "            num_ex = 1\n",
    "        print(candidate)\n",
    "        \n",
    "        if os.path.isfile(path_to_contexts_wout_replacements + key + '_' + candidate + '.txt'):\n",
    "            with open(path_to_contexts_wout_replacements + key + '_' + candidate + '.txt', 'r', encoding='utf8') as f:\n",
    "                corpus_ex = f.read()\n",
    "            corpus_ex = corpus_ex.split('\\n\\n')\n",
    "\n",
    "            corpus_ex_clean = list(set([re.sub(r'\\b'+candidate+r'\\b', '_'+ poly_word+ '_', i) for i in corpus_ex if i!='\\n']))\n",
    "\n",
    "            corpus_ex_clean = [i.replace('#', ' ').replace('-', ' ').replace('\\n', ' ') for i in corpus_ex_clean][:num_ex+1]\n",
    "\n",
    "            corpus_len_first = len(corpus_ex_clean)\n",
    "            print(corpus_len_first)\n",
    "\n",
    "            if total_len+corpus_len_first>desired_number_examples+10:\n",
    "\n",
    "                finish = desired_number_examples - total_len\n",
    "                corpus.extend(corpus_ex_clean[:finish])\n",
    "                corpus_len = len(corpus_ex_clean[:finish])\n",
    "\n",
    "            else:\n",
    "                corpus.extend(corpus_ex_clean)\n",
    "                corpus_len = len(corpus_ex_clean)\n",
    "                \n",
    "            total_len+=corpus_len\n",
    "        else:\n",
    "            print('MISSING', key+'_'+candidate)\n",
    "\n",
    "    with open(path_to_corpus_with_replaced_relats + key + '.txt', 'w', encoding='utf8') as f:\n",
    "        for sent in corpus:\n",
    "            f.write(sent+'\\n')\n",
    "            \n",
    "    length_total.append(key+'     '+str(len(corpus)))\n",
    "    print('---TOTAL_EXAMPLES---', '\\t', len(corpus), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Creating an xml-file with train and test sets (because the source code for training requires this type of input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SET THIS PARAM\n",
    "root_dir = path_to_corpus_with_replaced_relats\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "\n",
    "\n",
    "root = ET.Element(\"corpus\")\n",
    "doc = ET.SubElement(root, \"document\")\n",
    "par = ET.SubElement(doc, \"paragraph\")\n",
    "sent = ET.SubElement(par, \"sentence\")\n",
    "\n",
    "\n",
    "for poly_w in os.listdir(root_dir):\n",
    "    \n",
    "    with open(root_dir+'\\\\'+poly_w, 'r', encoding='utf8') as f:\n",
    "        text_corpus = f.readlines()\n",
    "    \n",
    "    for sentence in tqdm_notebook(text_corpus):\n",
    "        tokens = sentence.split()\n",
    "        \n",
    "        _bert_tokens = []\n",
    "        \n",
    "        for tok in tokens:\n",
    "            _tokens = tokenizer.tokenize(tok.strip('_'))\n",
    "            _bert_tokens.extend(_tokens)\n",
    "            \n",
    "        if len(_bert_tokens)<=509:\n",
    "        \n",
    "            for i in tokens:\n",
    "                if '_' not in i:\n",
    "                    word = ET.SubElement(sent, 'word')\n",
    "                    word.set('surface_form', i)\n",
    "                    word.set('lemma', i)\n",
    "                else:\n",
    "                    word = ET.SubElement(sent, 'word')\n",
    "                    word.set('surface_form', i.strip('_'))\n",
    "                    word.set('lemma', i.strip('_'))\n",
    "                    word.set('wn30_key', str(sense_map[poly_w]))\n",
    "            sent = ET.SubElement(par, \"sentence\")\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            num_bert_toks = 0\n",
    "\n",
    "            prev_bert_toks = []\n",
    "            follow_bert_toks = []\n",
    "            main_word_index = ''\n",
    "            stop_idx_back = None\n",
    "            stop_idx_forw = None\n",
    "\n",
    "            final_tokens = []\n",
    "            \n",
    "            for num, el in enumerate(tokens):\n",
    "                \n",
    "                if '_' in el:\n",
    "                    \n",
    "                    main_word_index = num\n",
    "                    \n",
    "                    break\n",
    "                    \n",
    "            num_bert_toks = len(tokenizer.tokenize(tokens[main_word_index]))\n",
    "                    \n",
    "            if main_word_index>150:\n",
    "                prev_bert_toks = tokens[main_word_index-151:main_word_index]\n",
    "            else:\n",
    "                prev_bert_toks = tokens[0:main_word_index]\n",
    "                \n",
    "            if len(tokens)>main_word_index+150:\n",
    "                follow_bert_toks = tokens[main_word_index+1:main_word_index+151]\n",
    "            else:\n",
    "                follow_bert_toks = tokens[main_word_index+1:]\n",
    "                \n",
    "            for num, el in reversed(list(enumerate(prev_bert_toks))):\n",
    "                if len(tokenizer.tokenize(el))+num_bert_toks>=509:\n",
    "                    stop_idx_back = num\n",
    "                    break\n",
    "                num_bert_toks+=len(tokenizer.tokenize(el))\n",
    "                                                    \n",
    "            if stop_idx_back==None:\n",
    "                for num, el in enumerate(follow_bert_toks):\n",
    "                    if len(tokenizer.tokenize(el))+num_bert_toks>=509:\n",
    "                        stop_idx_forw = num\n",
    "                        break\n",
    "                    num_bert_toks+=len(tokenizer.tokenize(el))\n",
    "                                    \n",
    "            if stop_idx_back!=None:\n",
    "                final_tokens.extend(prev_bert_toks[stop_idx_back+1:]+[tokens[main_word_index]])\n",
    "            elif stop_idx_forw!= None:\n",
    "                final_tokens.extend(prev_bert_toks+[tokens[main_word_index]]+follow_bert_toks[:stop_idx_forw])\n",
    "            else:\n",
    "                final_tokens.extend(prev_bert_toks+[tokens[main_word_index]]+follow_bert_toks)\n",
    "\n",
    "                \n",
    "                                    \n",
    "            for i in final_tokens:\n",
    "                if '_' not in i:\n",
    "                    word = ET.SubElement(sent, 'word')\n",
    "                    word.set('surface_form', i)\n",
    "                    word.set('lemma', i)\n",
    "                else:\n",
    "                    word = ET.SubElement(sent, 'word')\n",
    "                    word.set('surface_form', i.strip('_'))\n",
    "                    word.set('lemma', i.strip('_'))\n",
    "                    word.set('wn30_key', str(sense_map[poly_w]))\n",
    "            sent = ET.SubElement(par, \"sentence\")\n",
    "                \n",
    "                \n",
    "tree = ET.ElementTree(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save_train_xml_file = r\"C:\\Users\\Taiga_1billion\\Taiga_1billion\\train_taiga_proza_ru.xml\"\n",
    "xmlstr = minidom.parseString(ET.tostring(root)).toprettyxml(indent=\"   \")\n",
    "with open(path_to_save_train_xml_file, \"wb\") as f:\n",
    "    f.write(xmlstr.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_cleaning(sent):\n",
    "    brief_cleaning = re.sub(\"[^-А-Яа-яёA-Za-z-]+\", ' ', sent)\n",
    "    tokens = [w.lower() for w in brief_cleaning.split() if w.lower() not in stopwords_rus]     \n",
    "    txt = [morph.parse(token.lower().strip())[0].normal_form.strip(' ') for token in tokens]\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_map_russe = {'замок_1': '0',\n",
    " 'замок_2': '1',\n",
    " 'лук_1': '2',\n",
    " 'лук_2': '3',\n",
    "'бор_1':'4',\n",
    "'бор_2':'5',\n",
    "'дар_2.1': '7',\n",
    "'дар_2.3': '6',\n",
    " 'дар_1': '7',\n",
    " 'двигатель_1': '8',\n",
    " 'двигатель_2': '9',\n",
    " 'дедушка_2': '10',\n",
    " 'дедушка_1': '11',\n",
    " 'декрет_2': '12',\n",
    " 'декрет_1': '13',\n",
    " 'дерево_1': '14',\n",
    " 'дерево_2': '15',\n",
    " 'диалог_1': '16',\n",
    " 'диалог_2': '17',\n",
    "#  'диплом_1': '18',\n",
    " 'диплом_3': '19',\n",
    " 'диплом_2.1': '20',\n",
    " 'диплом_2.2': '20',\n",
    " 'доктор_2': '21',\n",
    " 'доктор_1': '22',\n",
    " 'жребий_2': '31',\n",
    " 'жребий_1': '30',\n",
    " 'доля_5': '25',\n",
    " 'доля_1.1': '24',\n",
    " 'доля_1.2': '24',\n",
    " 'доля_2.1': '24',\n",
    " 'доля_2.2': '24',\n",
    " 'доля_3': '24',\n",
    " 'доля_4': '24',\n",
    " 'достижение_2.1': '26',\n",
    " 'достижение_1.1': '27',\n",
    " 'достижение_2.2': '26',\n",
    " 'достижение_1.2': '27',\n",
    " 'жестокость_1': '28',\n",
    " 'жестокость_2': '29',\n",
    " 'затея_2': '32',\n",
    " 'затея_1': '33',\n",
    " 'застой_1': '34',\n",
    " 'застой_2': '35',\n",
    " 'затишье_2': '36',\n",
    " 'затишье_1': '37',\n",
    " 'затишье_3': '37',\n",
    " 'затмение_2': '38',\n",
    " 'затмение_1': '39'}\n",
    "\n",
    "sense_map_russe = {'ЗАМОК_СТРОЕНИЕ.txt': '0',\n",
    " 'ЗАМОК_ЗАПОР.txt': '1',\n",
    " 'ЛУК_ОРУЖИЕ.txt': '2',\n",
    " 'ЛУК_ОВОЩ.txt': '3',\n",
    " 'БОР_ЭЛЕМЕНТ.txt': '4',\n",
    " 'БОР_ЛЕС.txt': '5',\n",
    " 'ДАР_ВРОЖДЕННЫЙ.txt': '6',\n",
    " 'ДАР_ПОДАРОК.txt': '7',\n",
    " 'ДВИГАТЕЛЬ_АГРЕГАТ.txt': '8',\n",
    " 'ДВИГАТЕЛЬ_ДВИЖУЩ_СИЛА.txt': '9',\n",
    " 'ДЕДУШКА_СТАРИК.txt': '10',\n",
    " 'ДЕДУШКА_РОДСТВЕНН.txt': '11',\n",
    " 'ДЕКРЕТ_ОТПУСК.txt': '12',\n",
    " 'ДЕКРЕТ_ДОКУМЕНТ.txt': '13',\n",
    " 'ДЕРЕВО_ДЕРЕВЦЕ.txt': '14',\n",
    " 'ДЕРЕВО_ДРЕВЕСИНА.txt': '15',\n",
    " 'ДИАЛОГ_БЕСЕДА.txt': '16',\n",
    " 'ДИАЛОГ_МЖД_СТОРОНАМИ.txt': '17',\n",
    " 'ДИПЛОМ_ПАМЯТНЫЙ.txt': '19',\n",
    " 'ДИПЛОМ_РАБОТА.txt': '20',\n",
    " 'ДОКТОР_НАУК.txt': '21',\n",
    " 'ДОКТОР_ВРАЧ.txt': '22',\n",
    " 'ДОЛЯ_ЧАСТЬ.txt': '24',\n",
    " 'ДОЛЯ_УЧАСТЬ.txt': '25',\n",
    " 'ДОСТИЖЕНИЕ_ЦЕЛИ.txt': '26',\n",
    " 'ДОСТИЖЕНИЕ_УРОВНЯ.txt': '27',\n",
    " 'ЖЕСТОКОСТЬ_БЕСПОЩАДНОСТЬ.txt': '28',\n",
    " 'ЖЕСТОКОСТЬ_ОБРАЩЕНИЕ.txt': '29',\n",
    " 'ЖРЕБИЙ_РЕШЕНИЕ.txt': '30',\n",
    " 'ЖРЕБИЙ_СУДЬБА.txt': '31',\n",
    " 'ЗАТЕЯ_ЗАБАВА.txt': '32',\n",
    " 'ЗАТЕЯ_НАЧИНАНИЕ.txt': '33',\n",
    " 'ЗАСТОЙ_ЗАСТОЙН_ЯВЛЕНИЕ.txt': '34',\n",
    " 'ЗАСТОЙ_СТАГНАЦИЯ_РАЗВИТ.txt': '35',\n",
    " 'ЗАТИШЬЕ_СНИЖ_АКТИВНОСТИ.txt': '36',\n",
    " 'ЗАТИШЬЕ_БЕЗВЕТР_ТИШЬ.txt': '37',\n",
    " 'ЗАТМЕНИЕ_ОДУРЕНИЕ.txt': '38',\n",
    " 'ЗАТМЕНИЕ_СВЕТИЛА.txt': '39'}\n",
    "\n",
    "sense_map_russe_inv = {v : k for k, v in sense_map_russe.items()}\n",
    "\n",
    "dictionary_txt_map = {k : str(sense_map[sense_map_russe_inv[v]]) for k, v in dictionary_map_russe.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_russe_wiki = pd.read_csv(r'C:\\Users\\russe-wsi-kit\\data\\main\\wiki-wiki\\train.csv', sep = '\\t')\n",
    "test_russe_act = pd.read_csv(r'C:\\Users\\russe-wsi-kit\\data\\main\\active-dict\\train.csv', sep='\\t')\n",
    "test_russe_act = test_russe_act.append(test_russe_wiki, ignore_index=True)\n",
    "stop_list_all_candidates = ['ДИПЛОМ_ВУЗА']\n",
    "in_train_russe = [i.split('_')[0].lower() for i in sense_map.keys() if i[:-4] not in stop_list_all_candidates]\n",
    "in_train_russe = list(set(in_train_russe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in case of test dataset RUSSE\n",
    "absent_meaning = []\n",
    "\n",
    "root = ET.Element(\"corpus\")\n",
    "doc = ET.SubElement(root, \"document\")\n",
    "par = ET.SubElement(doc, \"paragraph\")\n",
    "sent = ET.SubElement(par, \"sentence\")\n",
    "\n",
    "for i in range(len(test_russe_act)):\n",
    "    if i != 48:\n",
    "        num_s = 0\n",
    "\n",
    "\n",
    "\n",
    "        if test_russe_act['word'].iloc[i] in in_train_russe:\n",
    "\n",
    "            full_context = test_russe_act['context'].iloc[i]\n",
    "\n",
    "\n",
    "            poly_w = test_russe_act['word'].iloc[i]\n",
    "            dict_sense = test_russe_act['gold_sense_id'].iloc[i]\n",
    "\n",
    "            if poly_w+'_'+str(dict_sense) in dictionary_txt_map.keys():\n",
    "\n",
    "                target_sense = dictionary_txt_map[poly_w+'_'+str(dict_sense)]\n",
    "\n",
    "                tokens = sent_cleaning(full_context)\n",
    "\n",
    "                for w in tokens:\n",
    "\n",
    "                    if w == 'лука':\n",
    "                        w = 'лук'\n",
    "                    if w == 'даром' and target_sense in ['10', '11', 10, 11]:\n",
    "                        w = 'дар'\n",
    "                    if w == 'доле' and target_sense in ['26', '27', 26, 27]:\n",
    "                        w = 'доля'\n",
    "                    if w == 'бора' and target_sense in ['4', '5', 4, 5]:\n",
    "                        w = 'бор'\n",
    "\n",
    "                    if w !=poly_w:\n",
    "\n",
    "                        word = ET.SubElement(sent, 'word')\n",
    "                        word.set('surface_form', w)\n",
    "                        word.set('lemma', w)\n",
    "\n",
    "                    else:\n",
    "                        word = ET.SubElement(sent, 'word')\n",
    "                        word.set('surface_form', poly_w)\n",
    "                        word.set('lemma', poly_w)\n",
    "                        word.set('wn30_key', target_sense)\n",
    "                        num_s+=1\n",
    "                if num_s == 0:\n",
    "                    print(i, poly_w, '\\n', full_context, '\\n', tokens, target_sense)\n",
    "                sent = ET.SubElement(par, \"sentence\")\n",
    "\n",
    "\n",
    "            else:\n",
    "                absent_meaning.append(poly_w+'_'+str(dict_sense))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mapping_toloka = {'акция_1':'1', #компании\n",
    "               'акция_2':'0', #действие\n",
    "               'байка_1':'3', #ткань\n",
    "               'байка_2':'2',\n",
    "               'гвоздика_1':'5',\n",
    "               'гвоздика_2':'4',\n",
    "               'гусеница_1':'6', #насекомое\n",
    "               'гусеница_2':'7', #механизм\n",
    "               'капот_1':'8', #машинный\n",
    "               'капот_2':'9', #одежда\n",
    "               'крона_1':'11',\n",
    "               'крона_2':'10',\n",
    "               'рок_1':'13',\n",
    "               'рок_2':'12',\n",
    "               'слог_1':'14',\n",
    "               'слог_2':'15',\n",
    "               'стопка_1':'16',\n",
    "               'стопка_2':'17',\n",
    "               'таз_1':'19',\n",
    "               'таз_2':'18',\n",
    "               'такса_1':'20',\n",
    "               'такса_2':'21'}\n",
    "\n",
    "sense_map_toloka = {'АКЦИЯ_ДЕЙСТВИЕ.txt': 0,\n",
    " 'АКЦИЯ_КОМПАНИИ.txt': 1,\n",
    " 'БАЙКА_ЛОЖЬ.txt': 2,\n",
    " 'БАЙКА_ТКАНЬ.txt': 3,\n",
    " 'ГВОЗДИКА_ПРИПРАВА.txt': 4,\n",
    " 'ГВОЗДИКА_РАСТЕНИЕ.txt': 5,\n",
    " 'ГУСЕНИЦА_ЛИЧИНКА.txt': 6,\n",
    " 'ГУСЕНИЦА_МЕХАНИЗМ.txt': 7,\n",
    " 'КАПОТ_МАШИНЫ.txt': 8,\n",
    " 'КАПОТ_ОДЕЖДА.txt': 9,\n",
    " 'КРОНА_ДЕНЬГИ.txt': 10,\n",
    " 'КРОНА_ДЕРЕВА.txt': 11,\n",
    " 'РОК_МУЗЫКА.txt': 12,\n",
    " 'РОК_СУДЬБА.txt': 13,\n",
    " 'СЛОГ_ЗВУК.txt': 14,\n",
    " 'СЛОГ_СТИЛЬ.txt': 15,\n",
    " 'СТОПКА_КУЧА.txt': 16,\n",
    " 'СТОПКА_ПОСУДА.txt': 17,\n",
    " 'ТАЗ_КОСТЬ.txt': 18,\n",
    " 'ТАЗ_ПОСУДА.txt': 19,\n",
    " 'ТАКСА_ОПЛАТА.txt': 20,\n",
    " 'ТАКСА_СОБАКА.txt': 21}\n",
    "\n",
    "sense_map_toloka_inv = {str(v) : k for k, v in sense_map_toloka.items()}\n",
    "\n",
    "dictionary_txt_map_toloka = {k : str(sense_map[sense_map_toloka_inv[v]]) for k, v in test_mapping_toloka.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2 = pd.read_csv(r'C:\\Users\\WordSenseRus\\WordSenseRus\\bts-rnc-crowd.tsv', sep='\\t')\n",
    "in_train_2 = ['акция', 'байка','гвоздика',  'гусеница', 'капот', 'крона','рок','слог','стопка','таз','такса']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(test_2)):\n",
    "    \n",
    "    if test_2['lemma'].iloc[i] in in_train_2:\n",
    "        \n",
    "        if pd.isna(test_2['left'].iloc[i]) != True:\n",
    "            input_left = sent_cleaning(test_2['left'].iloc[i])\n",
    "\n",
    "            for w in input_left:\n",
    "\n",
    "                word = ET.SubElement(sent, 'word')\n",
    "                word.set('surface_form', w)\n",
    "                word.set('lemma', w)\n",
    "        \n",
    "        sense_df = test_2['lemma'].iloc[i]+'_'+str(test_2['sense_id'].iloc[i])\n",
    "        if sense_df == 'байка_0':\n",
    "            continue\n",
    "        if sense_df in dictionary_txt_map_toloka.keys():\n",
    "            sense = dictionary_txt_map_toloka[sense_df]\n",
    "\n",
    "            word = ET.SubElement(sent, 'word')\n",
    "            word.set('surface_form', test_2['lemma'].iloc[i])\n",
    "            word.set('lemma', test_2['lemma'].iloc[i])\n",
    "            word.set('wn30_key', sense)\n",
    "\n",
    "            if pd.isna(test_2['right'].iloc[i]) != True:\n",
    "                input_right = sent_cleaning(test_2['right'].iloc[i])\n",
    "\n",
    "                for w in input_right:\n",
    "\n",
    "                    word = ET.SubElement(sent, 'word')\n",
    "                    word.set('surface_form', w)\n",
    "                    word.set('lemma', w)\n",
    "\n",
    "            sent = ET.SubElement(par, \"sentence\")\n",
    "\n",
    "    \n",
    "tree = ET.ElementTree(root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save_test_xml_file = r\"C:\\Users\\Taiga_1billion\\Taiga_1billion\\test_taiga_proza_ru_11_03.xml\"\n",
    "xmlstr = minidom.parseString(ET.tostring(root)).toprettyxml(indent=\"   \")\n",
    "with open(path_to_save_test_xml_file, \"wb\") as f:\n",
    "    f.write(xmlstr.encode('utf-8'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
